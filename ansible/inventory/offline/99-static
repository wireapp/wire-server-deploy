# In this section, add all machines in this installation.
#
# Ansible connects to the machine on `ansible_host`
#
# The machines talk to eachother on `ip`
#
# !!! if `ip` is not provided, ansible will default to the IP of the default
# interface. Which is probably not what you want
#
# <hostname> ansible_host=<internal_ip>
[all]
# kubenode1 ansible_host=100.89.110.8  ip=10.114.0.10
# kubenode2 ansible_host=100.154.219.107 ip=10.114.0.8
# kubenode3 ansible_host=100.227.143.169 ip=10.114.0.2
# You could add more if capacity is needed
# kubenode4 ....

# restund1 ansible_host=XXXX
# restund2 ansible_host=XXXX

# cassandra1 ansible_host=XXXX
# cassandra2 ansible_host=XXXX
# cassandra3 ansible_host=XXXX

# elasticsearch1 ansible_host=XXXX
# elasticsearch2 ansible_host=XXXX
# elasticsearch3 ansible_host=XXXX
#
# minio1 ansible_host=XXXX
# minio2 ansible_host=XXXX
# minio3 ansible_host=XXXX
#
# If you are in an offline environment, add an assethost here, from which
# artifacts are served
# assethost ansible_host=100.89.14.74 ip=10.114.0.9

# If you need to tunnel ssh connections through a bastion host (because your
# nodes are not directly reachable from the machine running ansible), define a
# bastion host as well, and uncomment the [bastion] section below.
#
# bastion ansible_host=XXXX ansible_user=some_user


# Below variables are set for all machines in the inventory.
[all:vars]
# If you need to ssh as a user that's not the same user as the one running ansible
# ansible_user=<some_user>
# ansible_password=<some password>
# ansible_sudo_pass=<some password>
# Keep in mind this user needs to be able to sudo passwordless.
# ansible_user = root
#
# Usually, you want to have a separate keypair to ssh to these boxes,
# and tell ansible where it is by setting `ansible_ssh_private_key_file`.
# ansible_ssh_private_key_file = ./dot_ssh/id_ed25519
# Note adding it to the ssh agent won't work in scenarios where ansible
# execution is wrapped through a container, as the ssh-agent socket isn't
# accessible there.

## In the next three sections, Cassandra, elasticsearch, and MinIO need
#  the name of the interface you want database services to run on
#  specified. While this can be used to specify a private network just
#  for databases, you must specify the interface name you want to run
#  services on, even in the case that it is the only interface on the box.

# Note: for offline configurations, this is required. the impact is that dns
# on the physical kubenodes does not perform requests against the kubernetes
# cluster, prefering the network provided DNS settings.
# resolvconf_mode: none

[cassandra:vars]
# cassandra_network_interface = enp1s0

[elasticsearch:vars]
# elasticsearch_network_interface = enp1s0

[minio:vars]
# minio_network_interface = enp1s0

### No longer used. generated by the nginz section of values/wire-server/values.yaml instead.
#prefix = "example-"
#domain = "example.com"
#deeplink_title = "example.com environment"

[restund:vars]
# Uncomment if your public IP is not on the default gateway
# restund_network_interface = enp1s0
# Uncomment and set to the true public IP if you are behind 1:1 NAT
# restund_peer_udp_advertise_addr = a.b.c.d
#
# Uncomment to create firewall exception for private networks
# restund_allowed_private_network_cidrs = a.b.c.d/24
# If you install restund together with other services on the same machine
# you need to restund_allowed_private_network_cidrs to allow these services
# to communicate on the private network. E.g. If your private network is 172.16.0.1/24
# restund_allowed_private_network_cidrs = 172.16.0/24

# Explicitely specify the restund user id to be "root" to override the default of "997"
restund_uid = root

# For the following groups, add all nodes defined above to the sections below.
# Define any additional variables that should be set for these nodes.

# Uncomment this is you use the bastion host
# [bastion]
# bastion

# Add all nodes that should be the master
[kube-master]
# kubenode1
# kubenode2
# kubenode3

[etcd]
# !!! There MUST be an UNEVEN amount of etcd servers
#
# Uncomment if etcd and kubernetes are colocated
#
# kubenode1 etcd_member_name=etcd1
# kubenode2 etcd_member_name=etcd2
# kubenode3 etcd_member_name=etcd3
#
# Uncomment if etcd cluster is separately deployed from kubernetes masters
# etcd1 etcd_member_name=etcd1
# etcd2 etcd_member_name=etcd2
# etcd3 etcd_member_name=etcd3

# Add all worker nodes here
[kube-node]
# kubenode1
# kubenode2
# kubenode3

# Additional worker nodes can be added
# You can label and annotate nodes. E.g. when deploying SFT you might want to
# deploy it only on certain nodes due to the public IP requirement.
# kubenode4 node_labels="{'wire.com/role': 'sftd'}" node_annotations="{'wire.com/external-ip': 'XXXX'}"
# kubenode5 node_labels="{'wire.com/role': 'sftd'}" node_annotations="{'wire.com/external-ip': 'XXXX'}"

# leave this group as is
[k8s-cluster:children]
kube-master
kube-node

[restund]

# restund1
# restund2

# Add all cassandra nodes here
[cassandra]
# cassandra1
# cassandra2
# cassandra3

# add a cassandra seed
[cassandra_seed]
# cassandra1

# Add all elasticsearch nodes here
[elasticsearch]
# elasticsearch1
# elasticsearch2
# elasticsearch3

# leave this as is
[elasticsearch_master:children]
elasticsearch

# Add all minio nodes here
[minio]
# minio1
# minio2
# minio3
